# 机器学习

Machine Learning 约等于 Looking for Function。

## Different types of Functions

- Regression: The function outputs a scalar.
- Classification:Given options(classes),the function outputs a correct one.
- Structured Learning: create someting with structure.

1. Function with unknown parameters **[$y = wx + b$]**
2. Define a loss function from training data **[$L(b, w)$]**
3. Optimization: find the parameters that minimize the loss function **[$argmin_{b,w} L(b, w)$]**

## gradient descent

梯度下降

![gradient descent](gradient_descent.png)

1. (randomly) Pick an initial point **[$w^0$]**
2. Compute $\partial L / \partial w |_{w=w^0}$
3. 如果计算的偏微分为负， 则增大 w, 如果为正，则减小 w

hyperparameters: 在优化过程中，需要自己设置调整的参数，比如学习率，学习步长等。

bias：偏差
piecewise：分段的
curves: 曲线的

## Sigmoid Function

$$
    y = c \ \frac{1}{1 + e^{-(b + wx_1)}}
    = c \ sigmoid(b + wx_1)
$$

对于分段函数

$$
    y = b + \sum_i c_i \ sigmoid(b_i  + w_i x_i)
$$

![函数运算的矩阵表示](sigmoid.png)
![sigmoid](sigmoid1.png)
![线性代数表示](线性代数表示.png)
![theta向量](theta.png)

batch: 批量
epoch: 把所有批次的数据全部训练一次就叫一次epoch。

![过拟合](过拟合.png)

## 神经网络

### create neuron network

```python

import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(10, 32)
            nn.sigmoid()
            nn.Linear(32, 1)
        )
    
    # 定义前向传播过程
    def forward(self, x):
        return self.net(x)
```

### loss function

损失函数用于衡量模型的预期值和真实值之间的差异。

常见的损失函数包括：

- 均方误差（MSELoss）：回归问题常用，计算输出和目标值的平方差
- 交叉熵损失（CrossEntropyLoss）：分类问题常用，计算输出和真实标签之间的交叉熵
- BCEWithLogitsLoss：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。

```python


```
